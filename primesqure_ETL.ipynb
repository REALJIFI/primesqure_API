{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5567b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv\n",
    "##!pip install requests\n",
    "#!pip install pandas\n",
    "#!pip install loguru\n",
    "#!pip install datetime\n",
    "#!pip install numpy\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc8c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import psycopg2\n",
    "from psycopg2.extras import Json, execute_values\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b9f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc955c",
   "metadata": {},
   "source": [
    "#### EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d072b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env (ensure your .env contains API_KEY=your_key)\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"API_KEY not found in environment. Put API_KEY=... in your .env file\")\n",
    "\n",
    "# Base URLs\n",
    "BASE_PROPERTY_URL = \"https://api.rentcast.io/v1/properties\"\n",
    "SALE_LISTING_URL = \"https://api.rentcast.io/v1/listings/sale\"\n",
    "\n",
    "# Common headers\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x-api-key\": API_KEY\n",
    "}\n",
    "\n",
    "# Storage\n",
    "property_records = []\n",
    "sale_listing_records = []\n",
    "\n",
    "def safe_get_json(url, params=None, headers=None, timeout=20):\n",
    "    \"\"\"\n",
    "    Request JSON and return parsed JSON (or None on failure).\n",
    "    Preferably the API returns a list; we return what .json() gives and caller should validate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error for {url} with params {params}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Non-200 response ({resp.status_code}) for {url} with params {params}: {resp.text[:200]}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return resp.json()\n",
    "    except ValueError:\n",
    "        print(f\"Failed to parse JSON for {url} with params {params}\")\n",
    "        return None\n",
    "\n",
    "# Step 1: Fetch sale listings for a broad area to get addresses\n",
    "# Example: Query by city (e.g., \"San Antonio, TX\") or zip code. Adjust as needed.\n",
    "search_params = {\n",
    "    \"zipCode\": \"78204\",  # You can change this to another city, zip code, or other supported parameter\n",
    "    \"limit\": 50  # Fetch up to 50 listings to ensure we get enough unique addresses\n",
    "}\n",
    "\n",
    "print(f\"\\n=== Fetching sale listings for {search_params['zipCode']} ===\")\n",
    "sale_listings = safe_get_json(SALE_LISTING_URL, params=search_params, headers=headers)\n",
    "\n",
    "# Step 2: Extract unique addresses from sale listings\n",
    "addresses = []\n",
    "if sale_listings and isinstance(sale_listings, list):\n",
    "    for listing in sale_listings:\n",
    "        address = listing.get(\"formattedAddress\")\n",
    "        if address and address not in addresses:\n",
    "            addresses.append(address)\n",
    "            if len(addresses) >= 10:  # Stop at 10 unique addresses\n",
    "                break\n",
    "    print(f\" -> Found {len(addresses)} unique addresses\")\n",
    "else:\n",
    "    print(\" -> No sale listings returned\")\n",
    "    addresses = []\n",
    "\n",
    "# Step 3: Fetch property and sale listing data for each address\n",
    "for address in addresses[:10]:  # Limit to 10 addresses\n",
    "    print(f\"\\n=== Getting data for: {address} ===\")\n",
    "    params = {\"address\": address}\n",
    "\n",
    "    # Get property data\n",
    "    prop_json = safe_get_json(BASE_PROPERTY_URL, params=params, headers=headers)\n",
    "    if prop_json and isinstance(prop_json, list) and len(prop_json) > 0:\n",
    "        property_records.append(prop_json[0])\n",
    "        print(\" -> Property data appended\")\n",
    "    else:\n",
    "        print(\" -> No property data returned\")\n",
    "\n",
    "    # Get sale listing data\n",
    "    sale_json = safe_get_json(SALE_LISTING_URL, params=params, headers=headers)\n",
    "    if sale_json and isinstance(sale_json, list) and len(sale_json) > 0:\n",
    "        sale_listing_records.append(sale_json[0])\n",
    "        print(\" -> Sale listing appended\")\n",
    "    else:\n",
    "        print(\" -> No sale listing returned\")\n",
    "\n",
    "    # Small pause to be polite to the API\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf360068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Data extraction complete.\n",
      "Properties rows: 10\n",
      "Sale listings rows: 10\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrames (safe: check empty lists)\n",
    "df_properties = pd.json_normalize(property_records) if property_records else pd.DataFrame()\n",
    "df_sale_listings = pd.json_normalize(sale_listing_records) if sale_listing_records else pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"\\n✅ Data extraction complete.\")\n",
    "print(\"Properties rows:\", len(df_properties))\n",
    "print(\"Sale listings rows:\", len(df_sale_listings))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32360622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show previews\n",
    "#df_properties.head()\n",
    "#df_sale_listings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f0b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get list of columns in a dataframe\n",
    "\n",
    "#list(df_sale_listings.columns)\n",
    "#list(df_properties.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a2e82",
   "metadata": {},
   "source": [
    "\n",
    "#### TRANSFORNATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4efa631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns you want first \n",
    "df_sale_listings = df_sale_listings[[\n",
    "    'id','status', 'price', 'listingType', 'listedDate','propertyType',\n",
    "    'listingAgent.name','listingAgent.phone','listingAgent.email',\n",
    "    'listingOffice.name', 'listingOffice.phone', 'listingOffice.email', 'listingOffice.website',\n",
    "    'removedDate','createdDate','lastSeenDate'\n",
    "]]\n",
    "\n",
    "df_properties = df_properties[[\n",
    "    'id','formattedAddress','county','lastSaleDate','owner.names','owner.type',\n",
    "    'lastSalePrice','ownerOccupied','bedrooms','bathrooms','squareFootage','yearBuilt',\n",
    "    'city','state','zipCode','latitude','longitude','propertyType','lotSize'\n",
    "]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d301692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) rename columns \n",
    "df_sale_listings = df_sale_listings.rename(columns={\n",
    "    \"id\": \"listing_code\",\n",
    "    \"listedDate\": \"listed_Date\",\n",
    "    \"listingType\": \"listing_Type\",\n",
    "    \"propertyType\": \"listing_property_Type\",\n",
    "    \"listingAgent.name\": \"agent_name\",\n",
    "    \"listingAgent.phone\": \"agent_phone\",\n",
    "    \"listingAgent.email\": \"agent_email\",\n",
    "    \"listingOffice.name\": \"listing_office_name\",\n",
    "    \"listingOffice.phone\": \"listing_office_phone\",\n",
    "    \"listingOffice.email\": \"listing_office_email\",\n",
    "    \"listingOffice.website\": \"listing_office_website\",\n",
    "    \"removedDate\": \"removed_Date\",\n",
    "    \"createdDate\": \"created_Date\",\n",
    "    \"lastSeenDate\": \"last_Seen_Date\"\n",
    "\n",
    "})\n",
    "\n",
    "df_properties = df_properties.rename(columns={\n",
    "    \"id\": \"property_code\",\n",
    "    \"propertyType\": \"property_Type\",\n",
    "    \"owner.names\": \"owner_names\",\n",
    "    \"owner.type\": \"owner_type\",\n",
    "    \"lastSaleDate\": \"last_saleDate\",\n",
    "    \"lastSalePrice\": \"last_SalePrice\",\n",
    "    \"ownerOccupied\": \"owner_Occupied\",\n",
    "    \"squareFootage\": \"square_Footage\",\n",
    "    \"yearBuilt\": \"year_Built\",\n",
    "    \"zipCode\": \"zip_Code\",\n",
    "    \"lotSize\": \"lot_Size\",\n",
    "    \"formattedAddress\": \"property_Address\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66a9ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date columns \n",
    "df_properties['last_saleDate'] = pd.to_datetime(df_properties['last_saleDate'], errors='coerce')\n",
    "\n",
    "# Convert multiple date columns at once\n",
    "date_cols = ['listed_Date', 'removed_Date', 'created_Date', 'last_Seen_Date']\n",
    "df_sale_listings[date_cols] = df_sale_listings[date_cols].apply(pd.to_datetime, errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b450b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING DATAFRAME INFORMATION\n",
    "\n",
    "#df_properties.info()\n",
    "#df_sale_listings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47315cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values\n",
    "\n",
    "# Fill rules for df_properties\n",
    "df_properties.fillna({\n",
    "    'bathrooms': 0.0,\n",
    "    'bedrooms': 0.0,\n",
    "    'square_Footage': 0.0,\n",
    "    'county': 'unknown',\n",
    "    'property_Type': 'unknown',\n",
    "    'year_Built': 0.0,  \n",
    "    'lot_Size': 0.0,\n",
    "    'owner_Occupied': 0.0,\n",
    "    'last_SalePrice': 0.0,\n",
    "    'owner_names': 'unknown',\n",
    "    'owner_type': 'unknown',\n",
    "    'Address': 'unknown',\n",
    "    'city': 'unknown',\n",
    "    'state': 'unknown',\n",
    "    'zip_Code': 'unknown',\n",
    "    'latitude': 0.0,\n",
    "    'longitude': 0.0,\n",
    "    'last_saleDate': pd.NaT\n",
    "}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "def575bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill rules for df_sale_listings\n",
    "df_sale_listings.fillna({\n",
    "    'status': 'unknown',\n",
    "    'price': 0.0,\n",
    "    'listing_Type': 'unknown',\n",
    "    'listed_Date': pd.NaT,\n",
    "    'listing_property_Type': 'unknown',\n",
    "    'agent_name': 'unknown',\n",
    "    'agent_phone': 'unknown',\n",
    "    'agent_email': 'unknown',\n",
    "    'listing_office_name': 'unknown',\n",
    "    'listing_office_phone': 'unknown',\n",
    "    'listing_office_email': 'unknown',\n",
    "    'listing_office_website': 'unknown',\n",
    "    'removed_Date': pd.NaT,\n",
    "    'created_Date': pd.NaT,\n",
    "    'last_Seen_Date': pd.NaT\n",
    "    \n",
    "}, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f543b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (10, 36)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1) Merge safely (use left to keep all properties; change to 'inner' if needed) ---\n",
    "df_merged = pd.merge(\n",
    "    df_properties,\n",
    "    df_sale_listings,\n",
    "    left_on='property_code',\n",
    "    right_on='listing_code',\n",
    "    how='left',\n",
    "    suffixes=('', '_listing'),\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "print(\"Merged shape:\", df_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12fd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e31219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate column\n",
    "df_merged.drop(['listing_property_Type'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "502149e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files saved: property_data.csv\n"
     ]
    }
   ],
   "source": [
    "# # Save to CSVs\n",
    "df_merged.to_csv(\"property_data.csv\", index=False)\n",
    "\n",
    "print(\"\\nFiles saved: property_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0189ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv so  you dont keep calling the api\n",
    "df_primesquare = pd.read_csv(\"property_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902246a",
   "metadata": {},
   "source": [
    "#### CREATE DIMENSION TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2254e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"cleaned_data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a16ace7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at: c:\\Users\\back2\\Desktop\\primesqure_API_Project\\src\\cleaned_data\n"
     ]
    }
   ],
   "source": [
    "location_dim_table = df_primesquare[['city', 'state', 'zip_Code','county',\n",
    "                              'longitude', 'latitude']].copy().drop_duplicates().reset_index(drop=True)\n",
    "# to add a column that was not in existence use the below code\n",
    "location_dim_table.index.name = 'location_id'\n",
    "# USE THIS CODE TO MAKE THE NEW CREATED INDEX TO APPEAR ON THE COLUMN HEADER\n",
    "location_dim_table = location_dim_table.reset_index()\n",
    "location_dim_table.to_csv(\"cleaned_data/location_dim.csv\", index=False)\n",
    "print(f\"CSV file created at: {os.path.abspath('cleaned_data')}\")\n",
    "#print(location_dim_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2a6214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_dim_table = df_primesquare[['property_code', 'property_Address', 'property_Type',\n",
    "                                'bedrooms', 'bathrooms', 'square_Footage', 'year_Built',\n",
    "                                'lot_Size']].copy().drop_duplicates().reset_index(drop=True)\n",
    "property_dim_table.index.name = 'property_id'\n",
    "property_dim_table = property_dim_table.reset_index()\n",
    "property_dim_table.to_csv(\"cleaned_data/property_dim_table.csv\", index=False)\n",
    "#print(property_dim_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24ca502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dim_table = df_primesquare[['agent_name', 'agent_phone', 'agent_email']].copy().drop_duplicates().reset_index(drop=True)\n",
    "agent_dim_table.index.name = 'agent_id'\n",
    "# USE THIS CODE TO MAKE THE NEW CREATED INDEX TO APPEAR ON THE COLUMN HEADER\n",
    "agent_dim_table = agent_dim_table.reset_index()\n",
    "agent_dim_table.to_csv(\"cleaned_data/agent_data.csv\" , index=False)\n",
    "#print(agent_dim_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d2b39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists in owner_names to strings\n",
    "df_primesquare['owner_names'] = df_primesquare['owner_names'].apply(\n",
    "    lambda x: ', '.join(x) if isinstance(x, list) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a35acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_dim_table = df_primesquare[['owner_names', 'owner_type', 'owner_Occupied']].copy().drop_duplicates().reset_index(drop=True)\n",
    "owner_dim_table.index.name = 'owner_id'\n",
    "# USE THIS CODE TO MAKE THE NEW CREATED INDEX TO APPEAR ON THE COLUMN HEADER\n",
    "owner_dim_table = owner_dim_table.reset_index()\n",
    "owner_dim_table.to_csv(\"cleaned_data/owner_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4e91fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "office_dim_table = df_primesquare[['listing_office_name', 'listing_office_phone', 'listing_office_email',\n",
    "                               'listing_office_website']].copy().drop_duplicates().reset_index(drop=True)\n",
    "office_dim_table.index.name = 'office_id'\n",
    "office_dim_table = office_dim_table.reset_index()\n",
    "office_dim_table.to_csv(\"cleaned_data/office_data.csv\", index=False)\n",
    "#print(office_dim_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0a0226fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_dim_table = df_primesquare[['listing_code','listing_Type',]].copy().drop_duplicates().reset_index(drop=True)\n",
    "listing_dim_table.index.name = 'listing_id'\n",
    "listing_dim_table = listing_dim_table.reset_index()\n",
    "listing_dim_table.to_csv(\"cleaned_data/listing_data.csv\", index=False)\n",
    "#print(listing_dim_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "859738b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the '_merge' column from df_primesquare to avoid confusion\n",
    "if '_merge' in df_primesquare.columns:\n",
    "    df_primesquare = df_primesquare.drop(columns=['_merge'])\n",
    "\n",
    "# Map foreign keys from dimension tables\n",
    "fact_dim_table = (df_primesquare.merge(property_dim_table, \n",
    "                                      on=['property_code', 'property_Address', 'property_Type', \n",
    "                                          'bedrooms', 'bathrooms', 'square_Footage', 'year_Built', 'lot_Size'], \n",
    "                                      how='left')\n",
    "                               .merge(owner_dim_table, \n",
    "                                      on=['owner_names', 'owner_type', 'owner_Occupied'], \n",
    "                                      how='left')\n",
    "                               .merge(location_dim_table, \n",
    "                                      on=['city', 'state', 'zip_Code', 'county', 'longitude', 'latitude'], \n",
    "                                      how='left')\n",
    "                               .merge(agent_dim_table, \n",
    "                                      on=['agent_name', 'agent_phone', 'agent_email'], \n",
    "                                      how='left')\n",
    "                               .merge(office_dim_table, \n",
    "                                      on=['listing_office_name', 'listing_office_phone', \n",
    "                                          'listing_office_email', 'listing_office_website'], \n",
    "                                      how='left')\n",
    "                               .merge(listing_dim_table, \n",
    "                                      on=['listing_code', 'listing_Type'], \n",
    "                                      how='left'))\n",
    "\n",
    "# Select columns from fact_dim_table (not df_primesquare)\n",
    "# Assuming dimension tables provide 'property_id', 'owner_id', etc.\n",
    "selected_columns = [\n",
    "    'property_id',  \n",
    "    'owner_id',     \n",
    "    'location_id',  \n",
    "    'agent_id',     \n",
    "    'office_id',    \n",
    "    'listing_id',   \n",
    "    'status', \n",
    "    'price', \n",
    "    'listing_Type', \n",
    "    'listed_Date', \n",
    "    'last_saleDate', \n",
    "    'removed_Date', \n",
    "    'created_Date', \n",
    "    'last_Seen_Date', \n",
    "    'property_Type',  \n",
    "    'last_SalePrice'\n",
    "]\n",
    "\n",
    "# Select available columns and handle missing ones\n",
    "available_columns = [col for col in selected_columns if col in fact_dim_table.columns]\n",
    "fact_dim_table = fact_dim_table[available_columns].copy()\n",
    "\n",
    "# Add fact_id if it doesn't exist\n",
    "if 'fact_id' not in fact_dim_table.columns:\n",
    "    fact_dim_table['fact_id'] = fact_dim_table.index + 1  # Simple incremental ID\n",
    "\n",
    "# Reorder columns to include fact_id first\n",
    "fact_dim_table = fact_dim_table[['fact_id'] + [col for col in available_columns if col != 'fact_id']]\n",
    "\n",
    "# Drop duplicates and reset index\n",
    "fact_dim_table = fact_dim_table.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Set fact_id as index name (if needed)\n",
    "fact_dim_table.index.name = 'fact_id'\n",
    "fact_dim_table.to_csv(\"cleaned_data/fact_data.csv\", index=False)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "#print(fact_dim_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba7d6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVELOPE A FUNCTION TO GET THE DATABASE CONNECTION\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def get_db_connection():\n",
    "    connection = psycopg2.connect(\n",
    "        host=os.getenv('P_host'),\n",
    "        database=os.getenv('P_database'),\n",
    "        user=os.getenv('p_user'),\n",
    "        password=os.getenv('P_password'),\n",
    "        port=os.getenv('P_port')  # Add the port here\n",
    "    ) \n",
    "    return connection\n",
    "\n",
    "# connect to database\n",
    "conn = get_db_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b35f4",
   "metadata": {},
   "source": [
    "#### Create pstgreSQL tables and schema for database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ac6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables():\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    create_table_query = '''\n",
    "        CREATE SCHEMA IF NOT EXISTS primesquare;\n",
    "\n",
    "        DROP TABLE IF EXISTS primesquare.property_dim_table CASCADE;\n",
    "        DROP TABLE IF EXISTS primesquare.owner_dim_table CASCADE;\n",
    "        DROP TABLE IF EXISTS primesquare.location_dim_table CASCADE;\n",
    "        DROP TABLE IF EXISTS primesquare.agent_dim_table CASCADE;\n",
    "        DROP TABLE IF EXISTS primesquare.office_dim_table CASCADE;\n",
    "        DROP TABLE IF EXISTS primesquare.listing_dim_table CASCADE;\n",
    "        DROP TABLE IF EXISTS primesquare.fact_dim_table CASCADE;\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS primesquare.property_dim_table (\n",
    "            property_id SERIAL PRIMARY KEY,\n",
    "            property_code VARCHAR(50) NOT NULL,\n",
    "            property_address VARCHAR(255) NOT NULL,\n",
    "            property_type VARCHAR(50),\n",
    "            bedrooms INTEGER,\n",
    "            bathrooms INTEGER,\n",
    "            square_footage INTEGER,\n",
    "            year_built INTEGER,\n",
    "            lot_size INTEGER,\n",
    "            UNIQUE (property_code, property_address, property_type, bedrooms, bathrooms, square_footage, year_built, lot_size)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS primesquare.owner_dim_table (\n",
    "            owner_id SERIAL PRIMARY KEY,\n",
    "            owner_names VARCHAR(500) NOT NULL,\n",
    "            owner_type VARCHAR(100),\n",
    "            owner_occupied BOOLEAN,\n",
    "            UNIQUE (owner_names, owner_type, owner_occupied)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS primesquare.location_dim_table (\n",
    "            location_id SERIAL PRIMARY KEY,\n",
    "            city VARCHAR(100),\n",
    "            state VARCHAR(50),\n",
    "            zip_code VARCHAR(20),\n",
    "            county VARCHAR(100),\n",
    "            longitude DOUBLE PRECISION,\n",
    "            latitude DOUBLE PRECISION,\n",
    "            UNIQUE (city, state, zip_code, county, longitude, latitude)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS primesquare.agent_dim_table (\n",
    "            agent_id SERIAL PRIMARY KEY,\n",
    "            agent_name VARCHAR(255) NOT NULL,\n",
    "            agent_phone VARCHAR(20),\n",
    "            agent_email VARCHAR(255),\n",
    "            UNIQUE (agent_name, agent_phone, agent_email)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS primesquare.office_dim_table (\n",
    "            office_id SERIAL PRIMARY KEY,\n",
    "            listing_office_name VARCHAR(255) NOT NULL,\n",
    "            listing_office_phone VARCHAR(20),\n",
    "            listing_office_email VARCHAR(255),\n",
    "            listing_office_website VARCHAR(255),\n",
    "            UNIQUE (listing_office_name, listing_office_phone, listing_office_email, listing_office_website)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS primesquare.listing_dim_table (\n",
    "            listing_id SERIAL PRIMARY KEY,\n",
    "            listing_code VARCHAR(50) NOT NULL,\n",
    "            listing_type VARCHAR(50),\n",
    "            UNIQUE (listing_code, listing_type)\n",
    "        );\n",
    "\n",
    "        CREATE TABLE IF NOT EXISTS primesquare.fact_dim_table (\n",
    "            fact_id SERIAL PRIMARY KEY,\n",
    "            property_id INTEGER NOT NULL,\n",
    "            owner_id INTEGER,\n",
    "            location_id INTEGER,\n",
    "            agent_id INTEGER,\n",
    "            office_id INTEGER,\n",
    "            listing_id INTEGER,\n",
    "            status VARCHAR(50),\n",
    "            price DECIMAL(15, 2),\n",
    "            listing_type VARCHAR(50),\n",
    "            listed_date TIMESTAMP,\n",
    "            last_saleDate TIMESTAMP,\n",
    "            removed_date TIMESTAMP,\n",
    "            created_date TIMESTAMP,\n",
    "            last_seen_date TIMESTAMP,\n",
    "            property_Type VARCHAR(100),\n",
    "            last_sale_price DECIMAL(15, 2),\n",
    "            FOREIGN KEY (property_id) REFERENCES primesquare.property_dim_table (property_id),\n",
    "            FOREIGN KEY (owner_id) REFERENCES primesquare.owner_dim_table (owner_id),\n",
    "            FOREIGN KEY (location_id) REFERENCES primesquare.location_dim_table (location_id),\n",
    "            FOREIGN KEY (agent_id) REFERENCES primesquare.agent_dim_table (agent_id),\n",
    "            FOREIGN KEY (office_id) REFERENCES primesquare.office_dim_table (office_id),\n",
    "            FOREIGN KEY (listing_id) REFERENCES primesquare.listing_dim_table (listing_id)\n",
    "        );\n",
    "\n",
    "        -- indexes (schema-qualified)\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_primesquare_property_id \n",
    "            ON primesquare.fact_dim_table (property_id);\n",
    "\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_primesquare_owner_id \n",
    "            ON primesquare.fact_dim_table (owner_id);\n",
    "\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_primesquare_location_id \n",
    "            ON primesquare.fact_dim_table (location_id);\n",
    "\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_primesquare_agent_id \n",
    "            ON primesquare.fact_dim_table (agent_id);\n",
    "\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_primesquare_office_id \n",
    "            ON primesquare.fact_dim_table (office_id);\n",
    "\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_primesquare_listing_id \n",
    "            ON primesquare.fact_dim_table (listing_id);\n",
    "\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_primesquare_status \n",
    "            ON primesquare.fact_dim_table (status);\n",
    "\n",
    "        CREATE INDEX IF NOT EXISTS idx_fact_primesquare_listed_date \n",
    "            ON primesquare.fact_dim_table (listed_date);\n",
    "    '''\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05ebd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae6ba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'primesquare_db' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Define database parameters including the database name\n",
    "# Load environment variables from .env file\n",
    "# Load environment variables from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define database parameters\n",
    "db_params = {\n",
    "    'P_user': os.getenv(\"P_user\"),\n",
    "    'P_password': os.getenv(\"P_password\"),\n",
    "    'P_host': os.getenv(\"P_host\"),\n",
    "    'P_port': os.getenv(\"P_port\"),\n",
    "    'P_database': os.getenv(\"P_database\")\n",
    "}\n",
    "\n",
    "# Define the default database URL for PostgreSQL\n",
    "default_db_url = f\"postgresql://{db_params['P_user']}:{db_params['P_password']}@{db_params['P_host']}:{db_params['P_port']}/postgres\"\n",
    "\n",
    "try:\n",
    "    # Connect to the default database\n",
    "    conn = psycopg2.connect(default_db_url)\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Check if the target database already exists\n",
    "    cur.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = %s\", (db_params['P_database'],))\n",
    "    exists = cur.fetchone()\n",
    "\n",
    "    if not exists:\n",
    "        # Create the target database if it doesn't exist\n",
    "        cur.execute(f\"CREATE DATABASE {db_params['P_database']}\")\n",
    "        print(f\"P_database '{db_params['database']}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Database '{db_params['P_database']}' already exists.\")\n",
    "\n",
    "    # Close cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ecb227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_user': postgres\n",
      "P_password: mypayment55\n",
      "P_host: localhost\n",
      "P_port: 5432\n",
      "P_database: primesquare_db\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Ensure this is executed\n",
    "\n",
    "# Debugging: Print loaded environment variables\n",
    "print(\"P_user':\", os.getenv(\"P_user\"))\n",
    "print(\"P_password:\", os.getenv(\"P_password\"))\n",
    "print(\"P_host:\", os.getenv(\"P_host\"))\n",
    "print(\"P_port:\", os.getenv(\"P_port\"))\n",
    "print(\"P_database:\", os.getenv(\"P_database\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9ab86",
   "metadata": {},
   "source": [
    "#### Insert data into database created tables and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3037250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def load_data_from_csv(csv_path):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # skip header\n",
    "        \n",
    "        for row in reader:\n",
    "            # Convert numeric fields to int or float as needed\n",
    "            property_id = int(row[0])\n",
    "            property_code = row[1]\n",
    "            property_address = row[2]\n",
    "            property_type = row[3]\n",
    "            bedrooms = int(float(row[4]))      # \"2.0\" -> 2\n",
    "            bathrooms = int(float(row[5]))     # \"1.0\" -> 1\n",
    "            square_footage = int(float(row[6])) # \"1200.0\" -> 1200\n",
    "            year_built = int(float(row[7]))\n",
    "            lot_size = float(row[8])           # keep float if needed\n",
    "            \n",
    "            cursor.execute(\n",
    "                '''\n",
    "                INSERT INTO primesquare.property_dim_table(\n",
    "                    property_id, property_code, property_address, property_type, \n",
    "                    bedrooms, bathrooms, square_footage, year_built, lot_size\n",
    "                )\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ''',\n",
    "                (property_id, property_code, property_address, property_type,\n",
    "                 bedrooms, bathrooms, square_footage, year_built, lot_size)\n",
    "            )\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "# Provide the CSV path\n",
    "csv_file_path = r'C:\\Users\\back2\\Desktop\\primesqure_API_Project\\src\\cleaned_data\\property_dim_table.csv'\n",
    "\n",
    "load_data_from_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6f84a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Owner data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_owner_dim_table_from_csv(csv_path):\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        owner_id = int(row['owner_id'])  # convert to int\n",
    "        owner_names = row['owner_names']\n",
    "        owner_type = row['owner_type']\n",
    "        \n",
    "        # Convert owner_occupied to boolean\n",
    "        if isinstance(row['owner_Occupied'], str):\n",
    "            owner_occupied = row['owner_Occupied'].strip().lower() in ['1', 'true', 'yes']\n",
    "        else:\n",
    "            owner_occupied = bool(row['owner_Occupied'])\n",
    "        \n",
    "        cursor.execute(\n",
    "            '''\n",
    "            INSERT INTO primesquare.owner_dim_table(\n",
    "                owner_id, owner_names, owner_type, owner_occupied\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ''',\n",
    "            (owner_id, owner_names, owner_type, owner_occupied)\n",
    "        )\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Owner data loaded successfully.\")\n",
    "\n",
    "# Provide the CSV path\n",
    "csv_file_path = r'C:\\Users\\back2\\Desktop\\primesqure_API_Project\\src\\cleaned_data\\owner_data.csv'\n",
    "\n",
    "load_owner_dim_table_from_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0ff3278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 10 rows into location_dim_table.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def load_location_dim_table_from_csv(csv_path):\n",
    "    try:\n",
    "        # Load CSV into DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Validate expected columns\n",
    "        expected_columns = ['location_id', 'city', 'state', 'zip_Code', 'county', 'longitude', 'latitude']\n",
    "        missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing columns in CSV: {missing_columns}\")\n",
    "        \n",
    "        # Convert data types\n",
    "        df['location_id'] = pd.to_numeric(df['location_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['zip_Code'] = df['zip_Code'].astype(str)  # Ensure zip_code is string to preserve leading zeros\n",
    "        df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce').fillna(0.0)\n",
    "        df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce').fillna(0.0)\n",
    "        \n",
    "        # Get database connection\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            # Prepare data for bulk insert\n",
    "            records = [\n",
    "                (\n",
    "                    row['location_id'],\n",
    "                    row['city'],\n",
    "                    row['state'],\n",
    "                    row['zip_Code'],\n",
    "                    row['county'],\n",
    "                    row['longitude'],\n",
    "                    row['latitude']\n",
    "                )\n",
    "                for _, row in df.iterrows()\n",
    "            ]\n",
    "            \n",
    "            # Bulk insert\n",
    "            execute_values(\n",
    "                cursor,\n",
    "                '''\n",
    "                INSERT INTO primesquare.location_dim_table(\n",
    "                    location_id, city, state, zip_code, county, longitude, latitude\n",
    "                )\n",
    "                VALUES %s\n",
    "                ''',\n",
    "                records\n",
    "            )\n",
    "            \n",
    "            conn.commit()\n",
    "            print(f\"Successfully loaded {len(records)} rows into location_dim_table.\")\n",
    "        \n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Database error: {e}\")\n",
    "            conn.rollback()\n",
    "            raise\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"CSV file not found: {csv_path}\")\n",
    "        raise\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"CSV file is empty: {csv_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "# Provide the CSV path\n",
    "csv_file_path = r'C:\\Users\\back2\\Desktop\\primesqure_API_Project\\src\\cleaned_data\\location_dim.csv'\n",
    "load_location_dim_table_from_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca5825e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_agent_dim_table_from_csv(csv_path):\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        agent_id = int(row['agent_id'])  # convert to int\n",
    "        agent_name = row['agent_name']\n",
    "        agent_phone = row['agent_phone']\n",
    "        agent_email = row['agent_email']\n",
    "        \n",
    "        cursor.execute(\n",
    "            '''\n",
    "            INSERT INTO primesquare.agent_dim_table(\n",
    "                agent_id, agent_name, agent_phone, agent_email\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            ''',\n",
    "            (agent_id, agent_name, agent_phone, agent_email)\n",
    "        )\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Agent data loaded successfully.\")\n",
    "\n",
    "# Provide the CSV path\n",
    "csv_file_path = r'C:\\Users\\back2\\Desktop\\primesqure_API_Project\\src\\cleaned_data\\agent_data.csv'\n",
    "\n",
    "load_agent_dim_table_from_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0e2e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Office data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_office_dim_table_from_csv(csv_path):\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        office_id = int(row['office_id'])  # convert to int\n",
    "        listing_office_name = row['listing_office_name']\n",
    "        listing_office_phone = row['listing_office_phone']\n",
    "        listing_office_email = row['listing_office_email']\n",
    "        listing_office_website = row['listing_office_website']\n",
    "        \n",
    "        cursor.execute(\n",
    "            '''\n",
    "            INSERT INTO primesquare.office_dim_table(\n",
    "                office_id, listing_office_name, listing_office_phone, listing_office_email, listing_office_website\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            ''',\n",
    "            (office_id, listing_office_name, listing_office_phone, listing_office_email, listing_office_website)\n",
    "        )\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Office data loaded successfully.\")\n",
    "\n",
    "# Provide the CSV path\n",
    "csv_file_path = r'C:\\Users\\back2\\Desktop\\primesqure_API_Project\\src\\cleaned_data\\office_data.csv'\n",
    "\n",
    "load_office_dim_table_from_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9250c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_listing_dim_table_from_csv(csv_path):\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        listing_id = int(row['listing_id'])  # convert to int\n",
    "        listing_code = row['listing_code']\n",
    "        listing_Type = row['listing_Type']\n",
    "        \n",
    "        cursor.execute(\n",
    "            '''\n",
    "            INSERT INTO primesquare.listing_dim_table(\n",
    "                listing_id, listing_code, listing_Type\n",
    "            )\n",
    "            VALUES (%s, %s, %s)\n",
    "            ''',\n",
    "            (listing_id, listing_code, listing_Type)\n",
    "        )\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Listing data loaded successfully.\")\n",
    "\n",
    "# Provide the CSV path\n",
    "csv_file_path = r'C:\\Users\\back2\\Desktop\\primesqure_API_Project\\src\\cleaned_data\\listing_data.csv'\n",
    "\n",
    "load_listing_dim_table_from_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31ba3e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 10 rows into fact_dim_table.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def load_fact_dim_table_from_csv(csv_path):\n",
    "    try:\n",
    "        # Load CSV into DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Validate expected columns\n",
    "        expected_columns = [\n",
    "            'fact_id', 'property_id', 'owner_id', 'location_id', 'agent_id', 'office_id', 'listing_id',\n",
    "            'status', 'price', 'listing_Type', 'listed_Date', 'last_saleDate', 'removed_Date',\n",
    "            'created_Date', 'last_Seen_Date', 'last_SalePrice', 'property_Type'\n",
    "        ]\n",
    "        missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing columns in CSV: {missing_columns}\")\n",
    "        \n",
    "        # Convert data types\n",
    "        df['fact_id'] = pd.to_numeric(df['fact_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['property_id'] = pd.to_numeric(df['property_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['owner_id'] = pd.to_numeric(df['owner_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['location_id'] = pd.to_numeric(df['location_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['agent_id'] = pd.to_numeric(df['agent_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['office_id'] = pd.to_numeric(df['office_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['listing_id'] = pd.to_numeric(df['listing_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(pd.NA)\n",
    "        df['last_SalePrice'] = pd.to_numeric(df['last_SalePrice'], errors='coerce').fillna(pd.NA)\n",
    "        \n",
    "        # Date fields: Ensure they are in a format compatible with PostgreSQL (e.g., YYYY-MM-DD)\n",
    "        date_columns = ['listed_Date', 'last_saleDate', 'removed_Date', 'created_Date', 'last_Seen_Date']\n",
    "        for col in date_columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "            df[col] = df[col].where(df[col].notnull(), None)\n",
    "        \n",
    "        # Get database connection\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            # Prepare data for bulk insert\n",
    "            records = [\n",
    "                (\n",
    "                    row['fact_id'],\n",
    "                    row['property_id'],\n",
    "                    row['owner_id'],\n",
    "                    row['location_id'],\n",
    "                    row['agent_id'],\n",
    "                    row['office_id'],\n",
    "                    row['listing_id'],\n",
    "                    row['status'],\n",
    "                    row['price'],\n",
    "                    row['listing_Type'],\n",
    "                    row['listed_Date'],\n",
    "                    row['last_saleDate'],\n",
    "                    row['removed_Date'],\n",
    "                    row['created_Date'],\n",
    "                    row['last_Seen_Date'],\n",
    "                    row['last_SalePrice'],\n",
    "                    row['property_Type']\n",
    "                )\n",
    "                for _, row in df.iterrows()\n",
    "            ]\n",
    "            \n",
    "            # Bulk insert (database columns use lowercase)\n",
    "            execute_values(\n",
    "                cursor,\n",
    "                '''\n",
    "                INSERT INTO primesquare.fact_dim_table(\n",
    "                    fact_id, property_id, owner_id, location_id, agent_id, office_id, listing_id,\n",
    "                    status, price, listing_type, listed_date, last_saledate, removed_date,\n",
    "                    created_date, last_seen_date, last_sale_price, property_type\n",
    "                )\n",
    "                VALUES %s\n",
    "                ''',\n",
    "                records\n",
    "            )\n",
    "            \n",
    "            conn.commit()\n",
    "            print(f\"Successfully loaded {len(records)} rows into fact_dim_table.\")\n",
    "        \n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Database error: {e}\")\n",
    "            conn.rollback()\n",
    "            raise\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"CSV file not found: {csv_path}\")\n",
    "        raise\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"CSV file is empty: {csv_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "# Provide the CSV path\n",
    "csv_file_path = r'C:\\Users\\back2\\Desktop\\primesqure_API_Project\\src\\cleaned_data\\fact_data.csv'\n",
    "load_fact_dim_table_from_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd278b",
   "metadata": {},
   "source": [
    "##### CALL STORED PROCEDURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a946d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"primesquare\",\n",
    "    user=\"postgres\",\n",
    "    password=\"your_password\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Call the procedure\n",
    "property_id = 101\n",
    "new_status = 'Removed'\n",
    "removed_date = datetime.now()\n",
    "\n",
    "cur.execute(\n",
    "    \"CALL primesquare.update_property_status(%s, %s, %s);\",\n",
    "    (property_id, new_status, removed_date)\n",
    ")\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
